{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "   1. pandas加载数据                                                                                 \n",
    "      1.1 熟悉数据集                                                                              \n",
    "      1.2 sklearn拆分数据集                                                                           \n",
    "      1.3 pandas df转为pyspark df                                                                       \n",
    "      1.3.1 熟悉sql.df的属性和方法                                                                       \n",
    "   2. 构建ALS模型                                                                                   \n",
    "   3. 为用户推荐评分Top-5的电影                                                                           \n",
    "       3.1 生成为所有训练集用户推荐的结果                                                                \n",
    "       3.2 为指定的用户和电影预测评分                                                                     \n",
    "   4. 评估模型                                                                                      \n",
    "   5. ALS参数不同取值，得到的均方根误差值汇总表如下：                                                          \n",
    "   6. 总结：\n",
    "   \n",
    "    ###  Action01项目要求：                                                                           \n",
    "           对MovieLens数据集进行评分预测\n",
    "           工具：可以使用Surprise或者其他                                                                 \n",
    "           说明使用的模型，及简要原理                                                                   \n",
    "           我们需要补全评分矩阵，然后对指定用户，比如userID为1-5进行预测                                           \n",
    "   #### jupyter中使用pyspark，需要先通过findspark运行。                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f8463984b777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# jupyter连接pyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'findspark' is not defined"
     ]
    }
   ],
   "source": [
    "# jupyter连接pyspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-49d7c4e178f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-14e7d2ab9523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # 回归器评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * # 导入它的目的不清楚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # 拆分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. pandas加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用pandas读取\n",
    "df = pd.read_csv('ratings.csv',\n",
    "                usecols=['userId','movieId','rating'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 熟悉数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = df['userId'].unique()\n",
    "movieid = df['movieId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid.shape,movieid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每部电影共被打分过多少次，默认按value降序\n",
    "temp = df['movieId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.set_index(keys='movieId',drop=False)\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按被打分次数由高到低重新提取数据集\n",
    "new_df = new_df.loc[temp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只被评分过一次的电影\n",
    "singles = new_df.iloc[-2558:]\n",
    "singles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 被评分过2次及以上的电影\n",
    "need_spilted = new_df.iloc[:-2558]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 sklearn拆分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark的sqldf.randomSplit()方法拆分数据集时，无法实现分层抽样。为了保证每个userid都可以被拆分到2个数据集中，所用sklearn的train_test_split进行拆分；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(need_splited[['userId','movieId']],\n",
    "                                                need_splited['rating'],\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=30,\n",
    "                                                stratify=need_spilted['movieId'])\n",
    "type(x_train),type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接被拆分的label和特征\n",
    "x_train['rating'] = y_train\n",
    "x_test['rating'] = y_test\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['userId'].unique().shape,x_test['userId'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['movieId'].unique().shape,x_test['movieId'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把被打分过一次的电影拼接进训练集中\n",
    "x_train_ = pd.concat((x_train,singles),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原数据集中用户id数和电影id数\n",
    "userid.shape, movieid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_['userId'].unique().shape,x_train_['movieId'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 拆分后训练集用户id数与总数据集用户id数相同，训练集的电影id与总数据集用户id数相同，说明训练集中已包含所有不同用户id和所有不同电影id。测试集用户id数、电影id数均少于训练集。但训练集中已包含所有用户id和电影id，因此可以确保测试集评估时不再出现空值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 pandas df 转为pyspark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_sc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 把pandas的DataFrame转为pyspark下的DataFrame\n",
    "spk_df_train = sql_sc.createDataFrame(data=x_train_)\n",
    "spk_df_test = sql_sc.createDataFrame(data=x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3.1 熟悉sql.df的属性和方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也是DataFrame类型，继承了pandas中部分df的方法\n",
    "type(spk_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据列信息\n",
    "spk_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看总行数和字段列表\n",
    "spk_df_train.count(), spk_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将每条记录转成Row对象，以列表形式返回\n",
    "# temp = spark_df_ratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练集前5条记录, 输出结果为sql数据表风格\n",
    "spk_df_train.show(n=5,truncate=True,vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看测试集前5条，take方法以列表形式返回Row对象\n",
    "spk_df_test.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#支持where条件语句查询\n",
    "spk_df_train.where(spk_df_train.userId==1).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 构建ALS模型\n",
    "ALS显示反馈目标函数：$$\\underset{x,y}{min}\\sum _{(u, i)\\varepsilon K}(r_{ui} - x_{u}^{T}y_{i})^2 + \\lambda (\\sum _{u}||x_{u}||_{2}^{2} + \\sum _{i}||y_{i}||_{2}^{2})$$\n",
    "\n",
    "$r_{ui}$实际评分                                                                                    \n",
    "$x_{u}^{T}y_{i}$用户向量与物品向量的内积，表示用户u 对物品i 的预测评分                                                \n",
    "$\\lambda (\\sum _{u}||x_{u}||_{2}^{2} + \\sum _{i}||y_{i}||_{2}^{2})$ L2正则项，保证数值计算稳定性，防止过拟合                      \n",
    "pyspark的API:                                                                                      \n",
    "\n",
    "    pyspark.ml.recommendation.ALS()                                                                           \n",
    "参数：                                                                                          \n",
    "     rank=10, 模型中潜在因子的数量（默认10，一般取10~1000，太小误差大；太大泛化能力差）                                         \n",
    "     maxIter=10, 要运行的最大迭代次数（ 默认为 10）                                                                 \n",
    "     regParam=0.1, 指定 ALS 中的正则化参数（ 默认为 0.1）                                                             \n",
    "numUserBlocks=10, 用户矩阵将被分区为块以便并行计算的块数（默认为10）                                                \n",
    "numItemBlocks=10, 项目矩阵将被分区为块以便并行计算的块数（默认为10)                                                   \n",
    "implicitPrefs=False, 指定是使用显式反馈ALS变体，还是使用适用于隐式反馈数据的变体；默认值 False，表示使用显式反馈                     \n",
    "alpha=1.0, 适用于 ALS 的隐式反馈变量的参数，其控制偏好观察中的基线置信度（ 默认为 1.0）                                    \n",
    "userCol='user', 用户列的字段名称，默认'user'                                                                \n",
    "itemCol='item', 项目列的字段名称，默认'item'                                                                \n",
    "seed=None,                                                                                         \n",
    "ratingCol='rating', 评分列的字段名称，默认'rating'                                                            \n",
    "nonnegative=False, 指定是否对最小二乘使用非负约束（ 默认False）                                                    \n",
    "checkpointInterval=10, 每10次迭代检查一次                                                                  \n",
    "intermediateStorageLevel='MEMORY_AND_DISK',                                                                   \n",
    "finalStorageLevel='MEMORY_AND_DISK',                                                                      \n",
    "coldStartStrategy='nan',                                                                              \n",
    "blockSize=4096                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "als = ALS(\n",
    "    rank=3,  \n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol='userId',\n",
    "    itemCol='movieId',\n",
    "    ratingCol='rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "model = als.fit(spk_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 为用户推荐评分Top-5的电影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 生成为所有训练集用户推荐的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 为所有训练集用户推荐Top-5\n",
    "recommends = model.recommendForAllUsers(numItems=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 查找模型生成的为userID=100推荐的top5电影明细\n",
    "user100 = recommends.where(recommends['userId'] == 100)\n",
    "user100.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(user100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user100.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['userId'] == 100][(df['movieId'] == 95218) | \\\n",
    "   (df['movieId'] == 40697) | (df['movieId'] == 4261) | (df['movieId'] ==82931)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询没结果，说明推荐的top-5电影都是用户以前未评分过的，ALS模型填充了评分矩阵中缺失的评分，使得这些评分能反应用户的喜好程度，从而让我们能利用这个预测评分排名后，选取了top-5部电影给指定用户。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 为指定的用户和电影预测评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sql_sc.createDataFrame([[100, 95218], \n",
    "                               [100, 40697],\n",
    "                               [100, 4261],\n",
    "                               [100, 82931],\n",
    "                               [100, 95776]], \n",
    "                               ['userId', 'movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为指定用户和电影预测评分，按评分排序\n",
    "predictions = sorted(model.transform(test).collect(), \n",
    "                     key=lambda r: r[2], reverse=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 可以看到预测的评分值，与3.1中recommends=model.recommendForAllUsers()中的对应用户和电影的评分一致。不同的是，recommends中只保存了每个用户预测评分top5的电影；而model.transform()可以为任何指定的用户id和电影id预测其评分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取df中前5条数据进行预测, 按movieId升序排序\n",
    "test = sql_sc.createDataFrame(df.iloc[:5])\n",
    "\n",
    "sorted(model.transform(test).collect(), \n",
    "       key=lambda r: r[1], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以看出预测值和实际值之间有出入，引入回归评估器评估预测效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(spk_df_test)\n",
    "preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据列信息\n",
    "preds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 测试集记录数209204，非空值209204，说明出现0条空值记录\n",
    "preds.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建回归评估器，评估误差\n",
    "evaluator = RegressionEvaluator(metricName='rmse', \n",
    "                                predictionCol='prediction',\n",
    "                                labelCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rms = evaluator.evaluate(preds.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 0.8395\n",
    "\"均方根误差值: {:.4f}\".format(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 ALS参数不同取值，得到的均方根误差值汇总表如下：\n",
    "序号\trank\tmaxIter\tregParam\trmse\n",
    "1\t3\t10\t0.01\t0.8528\n",
    "2\t3\t10\t0.1\t0.8395\n",
    "3\t3\t15\t0.1\t0.8370\n",
    "4\t3\t20\t0.1\t0.8376\n",
    "5\t3\t10\t0.5\t0.9983\n",
    "6\t10\t10\t0.5\t0.9982\n",
    "7\t10\t10\t0.1\t0.8239\n",
    "8\t20\t10\t0.1\t0.8219\n",
    "9\t20\t20\t0.1\t0.8162\n",
    "10\t50\t20\t0.1\t0.8153\n",
    "11\t3\t10\t2\t2.1745\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8153 / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 总结：\n",
    "    关于ALS算法：\n",
    "它通过userCol指定的用户列, 和itemCol指定的项目列，由implicitPrefs指定处理的是显示反馈还是隐式行为的场景，根据rank指定的隐特征个数拟合评分矩阵；拟合后再通过recommendForAllUsers(N)为所有用户生成评分最高的Top-N的推荐结果，根据指定的用户id即可在生成的结果中查找推荐结果。\n",
    "\n",
    "pyspark进行拟合时，ALS的rank, maxIter, regParam三个参数的取值会影响rmse。\n",
    "\n",
    "当rank太小时，单纯提高迭代次数maxIter很快会过拟合。\n",
    "\n",
    "汇总表序号2和序号10的结果可以看出，rank、maxIter同时提高取值，rmse会更好的降低；但增大rank和maxIter耗时会增加，rank取值1000的时候，跑了1个多小时仍没跑完。\n",
    "\n",
    "    其他注意事项：\n",
    "        1.必须保证训练集中包含所有的用户id和电影id，即：要保证训练集样本覆盖全部用户和电影，如：该数据集中共有7120个不同用户、14026部不同电影，则训练集中必须包含7120个用户和14026部电影下的部分数据；\n",
    "        2.如果训练集中未覆盖全部用户和电影，则训练集评估时预测值将会出现空值，导致rmse也是空值，影响拟合；\n",
    "        3.虽然真实数据集中rating的值在[0.5, 5.0]范围，而预测评分会出现大于5.0的情况。经过多次尝试，发现当rmse误差值降低后，大于5.0的评分情况会减少，所以猜测是因为误差过大导致（最低rmse占最高真实评分5.0的16.31%）；\n",
    "        4.预测结果DataFrame的字段prediction数据列信息是: float (nullable = false)，当nullable = false不代表一定原空值;\n",
    "        5.SQLContext是Spark SQL进行结构化数据处理的入口，它兼具了pandas的DataFrame的大部分方法和SQL语句，实际应用中可以多尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
